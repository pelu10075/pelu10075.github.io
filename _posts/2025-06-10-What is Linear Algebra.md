---
layout: post
title: What is Linear Algebra?
date: 2025-06-10 14:10:00
description: Introduction to Linear Algebra
tags: LinearAlgebra EN
categories: Study
featured: false
---

Linear algebra is a branch of mathematics centered around **vectors**, **matrices**, and **linear transformations**. These theories are not confined to math textbooks—they are actively used in fields like **computer science**, **economics**, and **natural sciences**.

In particular, **computer science** relies on linear algebra as a foundational component for core technologies such as artificial intelligence, graphics, and data analysis.

In this blog series, I aim to **explain the concepts and applications of linear algebra from a computer science perspective**, in an approachable way. Even if you're not familiar with mathematics, the posts will be structured to help you understand through real-world applications and examples.

Since there’s some space left in this introductory post, I’ll also cover the historical background of linear algebra, its applications, and an outline of future posts.

---

## Historical Background of Linear Algebra

Although linear algebra is a key part of modern mathematics, its roots go back as far as 3000 BCE. The terminology of vectors and matrices is relatively modern, but the ideas themselves existed long ago.

### 1. Ancient Babylon (~3000 BCE)

The Babylonians were capable of solving systems of equations up to the third degree. While they didn’t use symbolic notation, they had algorithms equivalent to solving linear equations.

- Source: Victor J. Katz, _A History of Mathematics: An Introduction_, Addison-Wesley, 2009.

### 2. Ancient China – _The Nine Chapters on the Mathematical Art_ (~2nd century BCE)

The chapter "Fangcheng" in _The Nine Chapters_ describes a method for solving systems of linear equations, similar to Gaussian elimination.

- Source: Joseph Needham, _Science and Civilisation in China, Volume 3_, Cambridge University Press, 1959.

### 3. 18th–19th Century Europe – Birth of Matrix Theory

**Gabriel Cramer (1750)**: Introduced Cramer's Rule for solving systems of linear equations.

**Carl Friedrich Gauss (1800s)**: Formalized the **Gaussian elimination** method.

**Augustin-Louis Cauchy (1820s)**: Organized the concepts of matrices and determinants.

**Arthur Cayley & James Sylvester (1850s)**: Formalized the concept of matrices and laid the groundwork for matrix operations and eigenvalue theory.

- Source: Thomas Hawkins, _Emergence of the Theory of Lie Groups_, Springer, 2000; Karen Hunger Parshall, _James Joseph Sylvester: Jewish Mathematician in a Victorian World_, Johns Hopkins University Press, 2006.

### 4. Development of Modern Linear Algebra (Early–Mid 20th Century)

Mathematicians such as **David Hilbert** and **Emmy Noether** connected linear algebra with **abstract algebra** and **functional analysis**, and it became a key language in **quantum mechanics**.

- Source: I. M. Gelfand, _Lectures on Linear Algebra_, Dover Publications, 1989.

---

## Why Linear Algebra Matters

1. Machine Learning / Deep Learning  
   Neural networks use vectors and matrices to represent weights, input data, and more.

2. Computer Graphics  
   3D transformations like rotation, scaling, and translation are represented using matrices.

3. Physics  
   In quantum mechanics, vector spaces are used to describe physical states.

4. Statistics / Data Analysis  
   Tools like covariance matrices and principal component analysis (PCA) rely heavily on linear algebra.

---

## Upcoming Posts

1. Common Terminology in Linear Algebra
2. Vectors and Matrices
3. Linear Transformations
4. Determinants
5. Cross Product and Dot Product
6. Eigenvalues and Eigenvectors
7. Solving Systems of Linear Equations
8. LU Decomposition
9. Gauss–Jordan Elimination
10. Eigenvalue Decomposition
11. Linear Regression

---

This post focused on summarizing the overall concepts and historical background of linear algebra. Starting with the next post, we will explore each topic in more detail.
